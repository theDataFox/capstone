{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Collector\n",
    "\n",
    "Collecting Pokémon images for all 20 classes from the internet and savingv in 'capstone/assets'. This will be accomplished using `BeautifulSoup`. Then remove any .gif images for the pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# possibly mnove to modules folder\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import urllib2\n",
    "import os\n",
    "import cookielib\n",
    "import json\n",
    "\n",
    "\n",
    "#load data of names for 20 Pokémon\n",
    "pokemon = pd.DataFrame({'pokemon' : [\"bulbasaur\", \"charmander\", \"squirtle\", \"pikachu\", \"dratini\", \"mew\", \"jolteon\", \n",
    "                                              \"vaporeon\", \"flareon\", \"eevee\", \"abra\", \"articuno\", \"zapdos\", \"moltres\",\n",
    "                                                 \"vulpix\", \"snorlax\", \"zubat\", \"pidgeot\", \"jigglypuff\", \"onix\"]})\n",
    "\n",
    "#define a function to read data file of names and search web and download images\n",
    "def download_images():\n",
    "    for name in pokemon.itertuples():\n",
    "        pokemonName = name[1]\n",
    "        def get_soup(url,header):\n",
    "            return BeautifulSoup(urllib2.urlopen(urllib2.Request(url,headers=header)),'html.parser')\n",
    "\n",
    "\n",
    "        query = pokemonName\n",
    "        image_type=\"photo\"\n",
    "        query= query.split()\n",
    "        query='+'.join(query)\n",
    "        url=\"https://www.google.com/search?q=\"+query+\"&source=lnms&tbm=isch&safe=active\"\n",
    "        print url\n",
    "        DIR=\"assets/train\"\n",
    "        header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"\n",
    "        }\n",
    "        soup = get_soup(url,header)\n",
    "\n",
    "\n",
    "        ActualImages=[]# contains the link for Large original images, type of  image\n",
    "        for a in soup.find_all(\"div\",{\"class\":\"rg_meta\"}):\n",
    "            link , Type =json.loads(a.text)[\"ou\"]  ,json.loads(a.text)[\"ity\"]\n",
    "            while len(ActualImages) <=50:\n",
    "                \n",
    "                ActualImages.append((link,Type))\n",
    "                break\n",
    "\n",
    "        #print  \"there are total\" , len(ActualImages),\"images\"\n",
    "\n",
    "        if not os.path.exists(DIR):\n",
    "                    os.mkdir(DIR)\n",
    "        DIR = os.path.join(DIR, query.split()[0])\n",
    "\n",
    "        if not os.path.exists(DIR):\n",
    "                    os.mkdir(DIR)\n",
    "        #print images\n",
    "        for i , (img , Type) in enumerate( ActualImages):\n",
    "            try:\n",
    "                req = urllib2.Request(img, headers={'User-Agent' : header})\n",
    "                raw_img = urllib2.urlopen(req).read()\n",
    "\n",
    "                cntr = len([i for i in os.listdir(DIR) if image_type in i]) + 1\n",
    "               # print cntr\n",
    "                if len(Type)==0:\n",
    "                    f = open(os.path.join(DIR , image_type + \"_\"+ str(cntr)+\".jpg\"), \"wb\")\n",
    "                else :\n",
    "                    f = open(os.path.join(DIR , image_type + \"_\"+ str(cntr)+\".\"+Type), \"wb\")\n",
    "\n",
    "\n",
    "                f.write(raw_img)\n",
    "                f.close()\n",
    "            except Exception as e:\n",
    "                print \"could not load : \"+img\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run image collection\n",
    "download_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy directories to validation dir\n",
    "import shutil\n",
    "shutil.copytree('assets/train', 'assets/validation')\n",
    "\n",
    "# leave only last 10 files in validation sub directories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through train dir and delete all .gif files\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"assets/train\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\".gif\")):\n",
    "            os.remove(os.path.join(root, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go through validation dir and delete all .gif files\n",
    "import os\n",
    "for root, dirs, files in os.walk(\"assets/validation\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\".gif\")):\n",
    "            os.remove(os.path.join(root, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
